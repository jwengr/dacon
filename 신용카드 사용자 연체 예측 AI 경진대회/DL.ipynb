{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL.ipynb",
      "provenance": [],
      "mount_file_id": "1fKrdcd0Z4Nb5vZoV6U7v3mB4fpKnkj-R",
      "authorship_tag": "ABX9TyN7//6IzPLpAqj3udDK3N3N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwengr/dacon/blob/main/%EC%8B%A0%EC%9A%A9%EC%B9%B4%EB%93%9C%20%EC%82%AC%EC%9A%A9%EC%9E%90%20%EC%97%B0%EC%B2%B4%20%EC%98%88%EC%B8%A1%20AI%20%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBnxQGUgBZH5"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMSbtbo28cdR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from itertools import combinations\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 564,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxmN0Zja8NMv"
      },
      "source": [
        "PATH = 'drive/My Drive/dacon/credit'"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1BAujN_8Zo1"
      },
      "source": [
        "train_df = pd.read_csv(PATH+'/dataset/train.csv').drop(['index','FLAG_MOBIL'],axis=1)\n",
        "test_df = pd.read_csv(PATH+'/dataset/test.csv').drop(['index','FLAG_MOBIL'],axis=1)"
      ],
      "execution_count": 699,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt6KyBMtAAeP"
      },
      "source": [
        "train_df['occyp_type'].fillna('one two three',inplace=True)\n",
        "test_df['occyp_type'].fillna('one two three',inplace=True)"
      ],
      "execution_count": 700,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys-gFWixd1TM"
      },
      "source": [
        "train_df['not_employed'] = (train_df['DAYS_EMPLOYED']>0).astype(np.int32)\n",
        "test_df['not_employed'] = (test_df['DAYS_EMPLOYED']>0).astype(np.int32)"
      ],
      "execution_count": 701,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVvMtMvVe_lf"
      },
      "source": [
        "train_df['begin_months'] = train_df['begin_month'].values%12\n",
        "test_df['begin_months'] = test_df['begin_month'].values%12"
      ],
      "execution_count": 702,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NU92txI8kVW"
      },
      "source": [
        "le = preprocessing.LabelEncoder()"
      ],
      "execution_count": 703,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBC2NrmW9Zmw",
        "outputId": "68212aaa-118b-4ad5-e7f6-6fe4dae35847"
      },
      "source": [
        "for col in ['gender','car','reality','work_phone','phone','email','not_employed','begin_months','family_size']:\n",
        "    train_df[[col]] = le.fit_transform(train_df[[col]]).astype(np.int32)\n",
        "    train_df = pd.concat([train_df,pd.get_dummies(train_df[col],prefix=f'{col}')],axis=1)\n",
        "\n",
        "    test_df[[col]] = le.fit_transform(test_df[[col]]).astype(np.int32)\n",
        "    test_df = pd.concat([test_df,pd.get_dummies(test_df[col],prefix=f'{col}')],axis=1)"
      ],
      "execution_count": 704,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGhXZjxa9s-p"
      },
      "source": [
        "for col in ['income_type','edu_type','family_type','house_type','occyp_type']:\n",
        "    train_df = pd.concat([train_df,pd.get_dummies(train_df[col],prefix=f'{col}')],axis=1)\n",
        "    test_df = pd.concat([test_df,pd.get_dummies(test_df[col],prefix=f'{col}')],axis=1)"
      ],
      "execution_count": 705,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfjU2fIyJQPp"
      },
      "source": [
        "dummies_df = train_df.iloc[:,34:]\n",
        "train_df = train_df.iloc[:,:34]\n",
        "\n",
        "test_dummies_df = pd.DataFrame(columns=dummies_df.columns)\n",
        "for col in test_df.iloc[:,33:].columns:\n",
        "    test_dummies_df[col] = test_df[col]\n",
        "test_dummies_df.fillna(0,inplace=True)\n",
        "test_df = test_df.iloc[:,:33]"
      ],
      "execution_count": 706,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Nq2y8NWYtA"
      },
      "source": [
        "dummies_arr = dummies_df.values\n",
        "test_dummies_arr = test_dummies_df.values"
      ],
      "execution_count": 707,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySlhGRrwVR7r",
        "outputId": "f0ee2928-29ce-4910-b00b-7e66630f1152"
      },
      "source": [
        "dummies_arr.shape,test_dummies_arr.shape"
      ],
      "execution_count": 708,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((26457, 62), (10000, 62))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 708
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHKJ3uXPI6hf"
      },
      "source": [
        "wide_arr = np.zeros((len(train_df),dummies_arr.shape[1]*(dummies_arr.shape[1]-1)//2),dtype=np.int32)\n",
        "test_wide_arr = np.zeros((len(test_df),test_dummies_arr.shape[1]*(test_dummies_arr.shape[1]-1)//2),dtype=np.int32)"
      ],
      "execution_count": 709,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ss1ABbqI2P1"
      },
      "source": [
        "i=0\n",
        "for a,b in list(combinations(range(dummies_arr.shape[1]),2)):\n",
        "    wide_arr[:,i] = dummies_arr[:,a]*dummies_arr[:,b]\n",
        "    i+=1\n",
        "i=0\n",
        "for a,b in list(combinations(range(test_dummies_arr.shape[1]),2)):\n",
        "    test_wide_arr[:,i] = test_dummies_arr[:,a]*test_dummies_arr[:,b]\n",
        "    i+=1"
      ],
      "execution_count": 710,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxtHHo7h9t5U"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 711,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v24NUx_fMmvQ",
        "outputId": "9bbddf8a-503c-4d81-b46f-5eb65ffaa810"
      },
      "source": [
        "print('----train----')\n",
        "for col in ['income_type','edu_type','family_type','house_type','occyp_type']:\n",
        "    print(col,max([len(tokenizer.encode(val)) for val in set(train_df[col].values)]))\n",
        "print('----test----')\n",
        "for col in ['income_type','edu_type','family_type','house_type','occyp_type']:\n",
        "    print(col,max([len(tokenizer.encode(val)) for val in set(test_df[col].values)]))"
      ],
      "execution_count": 712,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----train----\n",
            "income_type 4\n",
            "edu_type 6\n",
            "family_type 6\n",
            "house_type 6\n",
            "occyp_type 8\n",
            "----test----\n",
            "income_type 4\n",
            "edu_type 6\n",
            "family_type 6\n",
            "house_type 6\n",
            "occyp_type 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BFdgqOCCepa"
      },
      "source": [
        "emb_arr = np.zeros((len(train_df),3+5+5+5+7),np.int32)\n",
        "test_emb_arr = np.zeros((len(test_df),3+5+5+5+7),np.int32)"
      ],
      "execution_count": 713,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGZ9CkQjZ1JG"
      },
      "source": [
        "last=0\n",
        "for col,max_length in zip(['income_type','edu_type','family_type','house_type','occyp_type'],[4,6,6,6,8]):\n",
        "    emb_arr[:,last:last+max_length-1] = np.array(train_df[col].apply(lambda x: tokenizer.encode_plus(x,padding='max_length',max_length=max_length)['input_ids'][1:]).values.tolist())\n",
        "    last+=max_length-1\n",
        "last=0\n",
        "for col,max_length in zip(['income_type','edu_type','family_type','house_type','occyp_type'],[4,6,6,6,8]):\n",
        "    test_emb_arr[:,last:last+max_length-1] = np.array(test_df[col].apply(lambda x: tokenizer.encode_plus(x,padding='max_length',max_length=max_length)['input_ids'][1:]).values.tolist())\n",
        "    last+=max_length-1"
      ],
      "execution_count": 714,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ-AlykeapB6"
      },
      "source": [
        "for col in ['income_total','DAYS_BIRTH','DAYS_EMPLOYED','family_size','begin_month']:\n",
        "    scaler = StandardScaler()\n",
        "    train_df[f'{col}_scaled'] = scaler.fit_transform(train_df[[col]])\n",
        "    test_df[f'{col}_scaled'] = scaler.transform(test_df[[col]])"
      ],
      "execution_count": 715,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbbrE8B9dFcm"
      },
      "source": [
        "deep_arr = train_df[['gender','car','reality','child_num','work_phone','phone','email',\n",
        "          'income_total_scaled','DAYS_BIRTH_scaled','DAYS_EMPLOYED_scaled','family_size_scaled','begin_month_scaled']].values\n",
        "test_deep_arr = test_df[['gender','car','reality','child_num','work_phone','phone','email',\n",
        "          'income_total_scaled','DAYS_BIRTH_scaled','DAYS_EMPLOYED_scaled','family_size_scaled','begin_month_scaled']].values"
      ],
      "execution_count": 716,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_82ulDmpixmi"
      },
      "source": [
        "deep_arr = np.concatenate((deep_arr,emb_arr),axis=1)\n",
        "test_deep_arr = np.concatenate((test_deep_arr,test_emb_arr),axis=1)"
      ],
      "execution_count": 717,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG0RMtfcmlHD",
        "outputId": "22e1b64c-9658-4ec9-edf3-fd7921699e2d"
      },
      "source": [
        "deep_arr.shape,wide_arr.shape,test_deep_arr.shape,test_wide_arr.shape,"
      ],
      "execution_count": 718,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((26457, 37), (26457, 1891), (10000, 37), (10000, 1891))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 718
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHlJsKlnrJqR"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(((np.concatenate((deep_arr,wide_arr),axis=1)),pd.get_dummies(train_df['credit'].astype(np.int32)).values)).shuffle(26457)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(np.concatenate((test_deep_arr,test_wide_arr),axis=1)).batch(256)"
      ],
      "execution_count": 752,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJoQr3nita1S"
      },
      "source": [
        "train_dataset, val_dataset = dataset.take(int(26457*0.8)).batch(256), dataset.skip(int(26457*0.8)).batch(256)"
      ],
      "execution_count": 753,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ45ZdlSjwXH"
      },
      "source": [
        "inputs = keras.Input(shape=(deep_arr.shape[1]+wide_arr.shape[1]))\n",
        "deep,wide = Lambda( lambda x: tf.split(x,num_or_size_splits=[deep_arr.shape[1],wide_arr.shape[1]],axis=1))(inputs)\n",
        "\n",
        "deep = layers.BatchNormalization()(deep)\n",
        "deep = layers.Dense(32,activation='relu')(deep)\n",
        "deep = layers.Dense(12,activation='relu')(deep)\n",
        "deep = layers.Dense(3,activation='relu')(deep)\n",
        "\n",
        "\n",
        "wide = layers.BatchNormalization()(wide)\n",
        "wide = layers.Dense(3,activation='relu')(wide)\n",
        "\n",
        "\n",
        "x = layers.Concatenate()([deep,wide])\n",
        "outputs = layers.Dense(3,activation='softmax')(x)"
      ],
      "execution_count": 754,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM31zfW0qO0K",
        "outputId": "42bef724-61e6-4ce4-acdc-7021813068c3"
      },
      "source": [
        "model = keras.Model(inputs, outputs, name=\"wide_deep_model\")\n",
        "model.summary()"
      ],
      "execution_count": 755,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"wide_deep_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_58 (InputLayer)           [(None, 1928)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_59 (Lambda)              [(None, 37), (None,  0           input_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 37)           148         lambda_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_219 (Dense)               (None, 32)           1216        batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_220 (Dense)               (None, 12)           396         dense_219[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 1891)         7564        lambda_59[0][1]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_221 (Dense)               (None, 3)            39          dense_220[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_222 (Dense)               (None, 3)            5676        batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 6)            0           dense_221[0][0]                  \n",
            "                                                                 dense_222[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_223 (Dense)               (None, 3)            21          concatenate_42[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 15,060\n",
            "Trainable params: 11,204\n",
            "Non-trainable params: 3,856\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUM92zils08N",
        "outputId": "e5090c9f-666f-4087-ebe8-e1624a417f15"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=100,verbose=1)\n",
        "mc = ModelCheckpoint(PATH+'/model/wide_deep.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "history = model.fit(train_dataset,\n",
        "                        epochs=1000,\n",
        "                        verbose=1,\n",
        "                    callbacks=[early_stop, mc],\n",
        "                    validation_data=val_dataset\n",
        "                    )"
      ],
      "execution_count": 756,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "83/83 [==============================] - 2s 15ms/step - loss: 1.1687 - val_loss: 0.8703\n",
            "Epoch 2/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.8649 - val_loss: 0.8491\n",
            "Epoch 3/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.8610 - val_loss: 0.8535\n",
            "Epoch 4/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.8296 - val_loss: 0.8403\n",
            "Epoch 5/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.8334 - val_loss: 0.8171\n",
            "Epoch 6/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.8201 - val_loss: 0.8304\n",
            "Epoch 7/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.8093 - val_loss: 0.7933\n",
            "Epoch 8/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.8033 - val_loss: 0.7898\n",
            "Epoch 9/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.8008 - val_loss: 0.8152\n",
            "Epoch 10/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7920 - val_loss: 0.7947\n",
            "Epoch 11/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7934 - val_loss: 0.8058\n",
            "Epoch 12/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7911 - val_loss: 0.7637\n",
            "Epoch 13/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7909 - val_loss: 0.7641\n",
            "Epoch 14/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7865 - val_loss: 0.7830\n",
            "Epoch 15/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7826 - val_loss: 0.7738\n",
            "Epoch 16/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7710 - val_loss: 0.7779\n",
            "Epoch 17/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7842 - val_loss: 0.7563\n",
            "Epoch 18/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7653 - val_loss: 0.7639\n",
            "Epoch 19/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7649 - val_loss: 0.7627\n",
            "Epoch 20/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7608 - val_loss: 0.7617\n",
            "Epoch 21/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7677 - val_loss: 0.7597\n",
            "Epoch 22/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7664 - val_loss: 0.7677\n",
            "Epoch 23/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7590 - val_loss: 0.7595\n",
            "Epoch 24/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7634 - val_loss: 0.7559\n",
            "Epoch 25/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7638 - val_loss: 0.7699\n",
            "Epoch 26/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7537 - val_loss: 0.7455\n",
            "Epoch 27/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7620 - val_loss: 0.7483\n",
            "Epoch 28/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7590 - val_loss: 0.7478\n",
            "Epoch 29/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7646 - val_loss: 0.7363\n",
            "Epoch 30/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7601 - val_loss: 0.7516\n",
            "Epoch 31/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7500 - val_loss: 0.7344\n",
            "Epoch 32/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7510 - val_loss: 0.7577\n",
            "Epoch 33/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7529 - val_loss: 0.7411\n",
            "Epoch 34/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7467 - val_loss: 0.7407\n",
            "Epoch 35/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7527 - val_loss: 0.7308\n",
            "Epoch 36/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7461 - val_loss: 0.7371\n",
            "Epoch 37/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7503 - val_loss: 0.7309\n",
            "Epoch 38/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7391 - val_loss: 0.7239\n",
            "Epoch 39/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7407 - val_loss: 0.7374\n",
            "Epoch 40/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7351 - val_loss: 0.7370\n",
            "Epoch 41/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7412 - val_loss: 0.7447\n",
            "Epoch 42/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7478 - val_loss: 0.7407\n",
            "Epoch 43/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7374 - val_loss: 0.7347\n",
            "Epoch 44/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7422 - val_loss: 0.7366\n",
            "Epoch 45/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7339 - val_loss: 0.7213\n",
            "Epoch 46/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7397 - val_loss: 0.7377\n",
            "Epoch 47/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7344 - val_loss: 0.7349\n",
            "Epoch 48/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7329 - val_loss: 0.7505\n",
            "Epoch 49/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7336 - val_loss: 0.7235\n",
            "Epoch 50/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7303 - val_loss: 0.7393\n",
            "Epoch 51/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7323 - val_loss: 0.7441\n",
            "Epoch 52/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7228 - val_loss: 0.7272\n",
            "Epoch 53/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7288 - val_loss: 0.7259\n",
            "Epoch 54/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7332 - val_loss: 0.7177\n",
            "Epoch 55/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.7295 - val_loss: 0.7115\n",
            "Epoch 56/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7278 - val_loss: 0.7358\n",
            "Epoch 57/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.7320 - val_loss: 0.7237\n",
            "Epoch 58/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7340 - val_loss: 0.7198\n",
            "Epoch 59/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7277 - val_loss: 0.7269\n",
            "Epoch 60/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7322 - val_loss: 0.7269\n",
            "Epoch 61/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7268 - val_loss: 0.7232\n",
            "Epoch 62/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7331 - val_loss: 0.7162\n",
            "Epoch 63/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7359 - val_loss: 0.7153\n",
            "Epoch 64/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7302 - val_loss: 0.7096\n",
            "Epoch 65/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7235 - val_loss: 0.7135\n",
            "Epoch 66/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7302 - val_loss: 0.7232\n",
            "Epoch 67/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7213 - val_loss: 0.7244\n",
            "Epoch 68/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7253 - val_loss: 0.7204\n",
            "Epoch 69/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7222 - val_loss: 0.7287\n",
            "Epoch 70/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7222 - val_loss: 0.7219\n",
            "Epoch 71/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7269 - val_loss: 0.7219\n",
            "Epoch 72/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7154 - val_loss: 0.7144\n",
            "Epoch 73/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7274 - val_loss: 0.7181\n",
            "Epoch 74/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7293 - val_loss: 0.7094\n",
            "Epoch 75/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7227 - val_loss: 0.7073\n",
            "Epoch 76/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7228 - val_loss: 0.7325\n",
            "Epoch 77/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7244 - val_loss: 0.7066\n",
            "Epoch 78/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7215 - val_loss: 0.7234\n",
            "Epoch 79/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7202 - val_loss: 0.7282\n",
            "Epoch 80/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7186 - val_loss: 0.7228\n",
            "Epoch 81/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7234 - val_loss: 0.7246\n",
            "Epoch 82/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7240 - val_loss: 0.7125\n",
            "Epoch 83/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7224 - val_loss: 0.7101\n",
            "Epoch 84/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7287 - val_loss: 0.7127\n",
            "Epoch 85/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7162 - val_loss: 0.6940\n",
            "Epoch 86/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7210 - val_loss: 0.7160\n",
            "Epoch 87/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7169 - val_loss: 0.7282\n",
            "Epoch 88/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7107 - val_loss: 0.7160\n",
            "Epoch 89/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7162 - val_loss: 0.7143\n",
            "Epoch 90/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7227 - val_loss: 0.7100\n",
            "Epoch 91/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7217 - val_loss: 0.7155\n",
            "Epoch 92/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7162 - val_loss: 0.7050\n",
            "Epoch 93/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7179 - val_loss: 0.7265\n",
            "Epoch 94/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7126 - val_loss: 0.7207\n",
            "Epoch 95/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7116 - val_loss: 0.7095\n",
            "Epoch 96/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7197 - val_loss: 0.7049\n",
            "Epoch 97/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7125 - val_loss: 0.7026\n",
            "Epoch 98/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7160 - val_loss: 0.7126\n",
            "Epoch 99/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7174 - val_loss: 0.7198\n",
            "Epoch 100/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7192 - val_loss: 0.7170\n",
            "Epoch 101/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7182 - val_loss: 0.7020\n",
            "Epoch 102/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7164 - val_loss: 0.7059\n",
            "Epoch 103/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7105 - val_loss: 0.7018\n",
            "Epoch 104/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7153 - val_loss: 0.6982\n",
            "Epoch 105/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7058 - val_loss: 0.6900\n",
            "Epoch 106/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.7147 - val_loss: 0.7055\n",
            "Epoch 107/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7146 - val_loss: 0.7022\n",
            "Epoch 108/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7216 - val_loss: 0.7110\n",
            "Epoch 109/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7148 - val_loss: 0.7028\n",
            "Epoch 110/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7082 - val_loss: 0.7145\n",
            "Epoch 111/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7048 - val_loss: 0.7036\n",
            "Epoch 112/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7078 - val_loss: 0.7131\n",
            "Epoch 113/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7135 - val_loss: 0.6904\n",
            "Epoch 114/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7084 - val_loss: 0.6882\n",
            "Epoch 115/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7172 - val_loss: 0.7065\n",
            "Epoch 116/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7159 - val_loss: 0.6982\n",
            "Epoch 117/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7151 - val_loss: 0.7226\n",
            "Epoch 118/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.7118 - val_loss: 0.7003\n",
            "Epoch 119/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7071 - val_loss: 0.7037\n",
            "Epoch 120/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7027 - val_loss: 0.7019\n",
            "Epoch 121/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7051 - val_loss: 0.7030\n",
            "Epoch 122/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7112 - val_loss: 0.7232\n",
            "Epoch 123/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7057 - val_loss: 0.7056\n",
            "Epoch 124/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7103 - val_loss: 0.7191\n",
            "Epoch 125/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7080 - val_loss: 0.7030\n",
            "Epoch 126/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7050 - val_loss: 0.6862\n",
            "Epoch 127/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7094 - val_loss: 0.7077\n",
            "Epoch 128/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7152 - val_loss: 0.7020\n",
            "Epoch 129/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7089 - val_loss: 0.6914\n",
            "Epoch 130/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7081 - val_loss: 0.7023\n",
            "Epoch 131/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7117 - val_loss: 0.7001\n",
            "Epoch 132/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7035 - val_loss: 0.6963\n",
            "Epoch 133/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.7177 - val_loss: 0.6818\n",
            "Epoch 134/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7071 - val_loss: 0.7107\n",
            "Epoch 135/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7076 - val_loss: 0.6927\n",
            "Epoch 136/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7071 - val_loss: 0.7063\n",
            "Epoch 137/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7166 - val_loss: 0.6955\n",
            "Epoch 138/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7091 - val_loss: 0.7006\n",
            "Epoch 139/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.7098 - val_loss: 0.7011\n",
            "Epoch 140/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6984 - val_loss: 0.6966\n",
            "Epoch 141/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7116 - val_loss: 0.7013\n",
            "Epoch 142/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7090 - val_loss: 0.6986\n",
            "Epoch 143/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7099 - val_loss: 0.7022\n",
            "Epoch 144/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7041 - val_loss: 0.6921\n",
            "Epoch 145/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7002 - val_loss: 0.6961\n",
            "Epoch 146/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6993 - val_loss: 0.6979\n",
            "Epoch 147/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6980 - val_loss: 0.6860\n",
            "Epoch 148/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6970 - val_loss: 0.7066\n",
            "Epoch 149/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7065 - val_loss: 0.6746\n",
            "Epoch 150/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7021 - val_loss: 0.6940\n",
            "Epoch 151/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7194 - val_loss: 0.6909\n",
            "Epoch 152/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7054 - val_loss: 0.7055\n",
            "Epoch 153/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7053 - val_loss: 0.7006\n",
            "Epoch 154/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7088 - val_loss: 0.6897\n",
            "Epoch 155/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7105 - val_loss: 0.6817\n",
            "Epoch 156/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7005 - val_loss: 0.6994\n",
            "Epoch 157/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6970 - val_loss: 0.6940\n",
            "Epoch 158/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.7066 - val_loss: 0.7015\n",
            "Epoch 159/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6989 - val_loss: 0.6955\n",
            "Epoch 160/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7009 - val_loss: 0.6860\n",
            "Epoch 161/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7008 - val_loss: 0.7013\n",
            "Epoch 162/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.7096 - val_loss: 0.6928\n",
            "Epoch 163/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7053 - val_loss: 0.6950\n",
            "Epoch 164/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6981 - val_loss: 0.6968\n",
            "Epoch 165/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7006 - val_loss: 0.6964\n",
            "Epoch 166/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7089 - val_loss: 0.6836\n",
            "Epoch 167/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7047 - val_loss: 0.6833\n",
            "Epoch 168/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6959 - val_loss: 0.6943\n",
            "Epoch 169/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7078 - val_loss: 0.6988\n",
            "Epoch 170/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6998 - val_loss: 0.6967\n",
            "Epoch 171/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6945 - val_loss: 0.6816\n",
            "Epoch 172/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6942 - val_loss: 0.7100\n",
            "Epoch 173/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7026 - val_loss: 0.7076\n",
            "Epoch 174/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7048 - val_loss: 0.6777\n",
            "Epoch 175/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7022 - val_loss: 0.6846\n",
            "Epoch 176/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6965 - val_loss: 0.7099\n",
            "Epoch 177/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7028 - val_loss: 0.6946\n",
            "Epoch 178/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7099 - val_loss: 0.6959\n",
            "Epoch 179/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7021 - val_loss: 0.6960\n",
            "Epoch 180/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7071 - val_loss: 0.6950\n",
            "Epoch 181/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6955 - val_loss: 0.6931\n",
            "Epoch 182/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6951 - val_loss: 0.6777\n",
            "Epoch 183/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6926 - val_loss: 0.6880\n",
            "Epoch 184/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7071 - val_loss: 0.6840\n",
            "Epoch 185/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6978 - val_loss: 0.6951\n",
            "Epoch 186/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6946 - val_loss: 0.6836\n",
            "Epoch 187/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6998 - val_loss: 0.6609\n",
            "Epoch 188/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7058 - val_loss: 0.6936\n",
            "Epoch 189/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6967 - val_loss: 0.6833\n",
            "Epoch 190/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7068 - val_loss: 0.6942\n",
            "Epoch 191/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7010 - val_loss: 0.7054\n",
            "Epoch 192/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6979 - val_loss: 0.7033\n",
            "Epoch 193/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6938 - val_loss: 0.7071\n",
            "Epoch 194/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6949 - val_loss: 0.6958\n",
            "Epoch 195/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6930 - val_loss: 0.6845\n",
            "Epoch 196/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6851 - val_loss: 0.6968\n",
            "Epoch 197/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7014 - val_loss: 0.6964\n",
            "Epoch 198/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6991 - val_loss: 0.6911\n",
            "Epoch 199/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7024 - val_loss: 0.6943\n",
            "Epoch 200/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6946 - val_loss: 0.6941\n",
            "Epoch 201/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7094 - val_loss: 0.6815\n",
            "Epoch 202/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6900 - val_loss: 0.6808\n",
            "Epoch 203/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6915 - val_loss: 0.6956\n",
            "Epoch 204/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6945 - val_loss: 0.6835\n",
            "Epoch 205/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6946 - val_loss: 0.6865\n",
            "Epoch 206/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6956 - val_loss: 0.6867\n",
            "Epoch 207/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6873 - val_loss: 0.6873\n",
            "Epoch 208/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7042 - val_loss: 0.6794\n",
            "Epoch 209/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6914 - val_loss: 0.7004\n",
            "Epoch 210/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6924 - val_loss: 0.6947\n",
            "Epoch 211/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6836 - val_loss: 0.7000\n",
            "Epoch 212/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6948 - val_loss: 0.6879\n",
            "Epoch 213/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6878 - val_loss: 0.6887\n",
            "Epoch 214/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6901 - val_loss: 0.6937\n",
            "Epoch 215/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6987 - val_loss: 0.6809\n",
            "Epoch 216/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6781 - val_loss: 0.6887\n",
            "Epoch 217/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6964 - val_loss: 0.6952\n",
            "Epoch 218/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7027 - val_loss: 0.7018\n",
            "Epoch 219/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6943 - val_loss: 0.6883\n",
            "Epoch 220/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6935 - val_loss: 0.6945\n",
            "Epoch 221/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6998 - val_loss: 0.6902\n",
            "Epoch 222/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6973 - val_loss: 0.6971\n",
            "Epoch 223/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6922 - val_loss: 0.6854\n",
            "Epoch 224/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6972 - val_loss: 0.7188\n",
            "Epoch 225/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.6911 - val_loss: 0.6856\n",
            "Epoch 226/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6975 - val_loss: 0.6867\n",
            "Epoch 227/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6938 - val_loss: 0.6762\n",
            "Epoch 228/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6881 - val_loss: 0.7108\n",
            "Epoch 229/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6987 - val_loss: 0.6901\n",
            "Epoch 230/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6902 - val_loss: 0.6976\n",
            "Epoch 231/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6963 - val_loss: 0.6851\n",
            "Epoch 232/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7003 - val_loss: 0.6731\n",
            "Epoch 233/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6863 - val_loss: 0.6852\n",
            "Epoch 234/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7044 - val_loss: 0.6868\n",
            "Epoch 235/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6958 - val_loss: 0.6838\n",
            "Epoch 236/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6920 - val_loss: 0.6904\n",
            "Epoch 237/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6947 - val_loss: 0.6887\n",
            "Epoch 238/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7002 - val_loss: 0.6871\n",
            "Epoch 239/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7085 - val_loss: 0.6885\n",
            "Epoch 240/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6976 - val_loss: 0.6759\n",
            "Epoch 241/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6946 - val_loss: 0.6717\n",
            "Epoch 242/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7087 - val_loss: 0.6884\n",
            "Epoch 243/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6992 - val_loss: 0.6977\n",
            "Epoch 244/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6983 - val_loss: 0.6800\n",
            "Epoch 245/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6995 - val_loss: 0.6918\n",
            "Epoch 246/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6930 - val_loss: 0.6850\n",
            "Epoch 247/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6924 - val_loss: 0.6894\n",
            "Epoch 248/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6863 - val_loss: 0.6858\n",
            "Epoch 249/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6992 - val_loss: 0.7057\n",
            "Epoch 250/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.6957 - val_loss: 0.6858\n",
            "Epoch 251/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6940 - val_loss: 0.6754\n",
            "Epoch 252/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.6904 - val_loss: 0.6830\n",
            "Epoch 253/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7037 - val_loss: 0.6920\n",
            "Epoch 254/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6926 - val_loss: 0.6879\n",
            "Epoch 255/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6908 - val_loss: 0.6766\n",
            "Epoch 256/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6916 - val_loss: 0.6919\n",
            "Epoch 257/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6920 - val_loss: 0.6932\n",
            "Epoch 258/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6909 - val_loss: 0.6761\n",
            "Epoch 259/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6919 - val_loss: 0.6830\n",
            "Epoch 260/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6988 - val_loss: 0.6682\n",
            "Epoch 261/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6943 - val_loss: 0.6830\n",
            "Epoch 262/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6945 - val_loss: 0.6779\n",
            "Epoch 263/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6992 - val_loss: 0.6778\n",
            "Epoch 264/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6848 - val_loss: 0.6926\n",
            "Epoch 265/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6854 - val_loss: 0.6804\n",
            "Epoch 266/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6906 - val_loss: 0.6939\n",
            "Epoch 267/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6952 - val_loss: 0.6802\n",
            "Epoch 268/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6957 - val_loss: 0.6802\n",
            "Epoch 269/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7007 - val_loss: 0.6860\n",
            "Epoch 270/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6902 - val_loss: 0.6840\n",
            "Epoch 271/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6860 - val_loss: 0.6884\n",
            "Epoch 272/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6974 - val_loss: 0.6868\n",
            "Epoch 273/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6994 - val_loss: 0.6973\n",
            "Epoch 274/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6926 - val_loss: 0.6821\n",
            "Epoch 275/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.6946 - val_loss: 0.6719\n",
            "Epoch 276/1000\n",
            "83/83 [==============================] - 1s 14ms/step - loss: 0.6858 - val_loss: 0.6874\n",
            "Epoch 277/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6808 - val_loss: 0.6879\n",
            "Epoch 278/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6906 - val_loss: 0.7054\n",
            "Epoch 279/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6867 - val_loss: 0.6913\n",
            "Epoch 280/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6975 - val_loss: 0.6798\n",
            "Epoch 281/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6927 - val_loss: 0.6844\n",
            "Epoch 282/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6916 - val_loss: 0.6816\n",
            "Epoch 283/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.7019 - val_loss: 0.6822\n",
            "Epoch 284/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6845 - val_loss: 0.7001\n",
            "Epoch 285/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6834 - val_loss: 0.6932\n",
            "Epoch 286/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6926 - val_loss: 0.6887\n",
            "Epoch 287/1000\n",
            "83/83 [==============================] - 1s 13ms/step - loss: 0.6882 - val_loss: 0.6796\n",
            "Epoch 00287: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "M1tC1qcYtrkk",
        "outputId": "9ebd5a74-5e30-489f-b881-0c64e76039f9"
      },
      "source": [
        "plt.plot(history.history['val_loss'])"
      ],
      "execution_count": 757,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcbdd68cd10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 757
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzd20AxsTPqm"
      },
      "source": [
        "saved_model = tf.keras.models.load_model(PATH+'/model/wide_deep.h5')"
      ],
      "execution_count": 758,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yB1lZlfXRNN"
      },
      "source": [
        "submission = pd.read_csv(PATH+'/dataset/sample_submission.csv')"
      ],
      "execution_count": 759,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1VThrg4XeD_"
      },
      "source": [
        "submission.iloc[:,1:] = saved_model.predict(test_dataset)"
      ],
      "execution_count": 760,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "glBD0mmNaN8F",
        "outputId": "518eb469-30ff-4501-a58b-4284b8a0805c"
      },
      "source": [
        "submission.head(20)"
      ],
      "execution_count": 761,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26457</td>\n",
              "      <td>0.024023</td>\n",
              "      <td>0.063343</td>\n",
              "      <td>9.126337e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>26458</td>\n",
              "      <td>0.133268</td>\n",
              "      <td>0.135334</td>\n",
              "      <td>7.313976e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26459</td>\n",
              "      <td>0.203205</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>6.031955e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26460</td>\n",
              "      <td>0.153787</td>\n",
              "      <td>0.173574</td>\n",
              "      <td>6.726397e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26461</td>\n",
              "      <td>0.186106</td>\n",
              "      <td>0.141842</td>\n",
              "      <td>6.720521e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>26462</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.554976</td>\n",
              "      <td>4.436972e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>26463</td>\n",
              "      <td>0.306671</td>\n",
              "      <td>0.685901</td>\n",
              "      <td>7.427885e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>26464</td>\n",
              "      <td>0.171500</td>\n",
              "      <td>0.207762</td>\n",
              "      <td>6.207376e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>26465</td>\n",
              "      <td>0.097556</td>\n",
              "      <td>0.134409</td>\n",
              "      <td>7.680354e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>26466</td>\n",
              "      <td>0.131565</td>\n",
              "      <td>0.169883</td>\n",
              "      <td>6.985524e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>26467</td>\n",
              "      <td>0.141042</td>\n",
              "      <td>0.151706</td>\n",
              "      <td>7.072527e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>26468</td>\n",
              "      <td>0.090468</td>\n",
              "      <td>0.172488</td>\n",
              "      <td>7.370440e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>26469</td>\n",
              "      <td>0.999525</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>1.332234e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>26470</td>\n",
              "      <td>0.124237</td>\n",
              "      <td>0.145297</td>\n",
              "      <td>7.304663e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>26471</td>\n",
              "      <td>0.113739</td>\n",
              "      <td>0.139498</td>\n",
              "      <td>7.467627e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>26472</td>\n",
              "      <td>0.096252</td>\n",
              "      <td>0.182203</td>\n",
              "      <td>7.215447e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>26473</td>\n",
              "      <td>0.216466</td>\n",
              "      <td>0.246271</td>\n",
              "      <td>5.372629e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>26474</td>\n",
              "      <td>0.296656</td>\n",
              "      <td>0.683857</td>\n",
              "      <td>1.948757e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>26475</td>\n",
              "      <td>0.325381</td>\n",
              "      <td>0.275219</td>\n",
              "      <td>3.994006e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>26476</td>\n",
              "      <td>0.016471</td>\n",
              "      <td>0.039693</td>\n",
              "      <td>9.438356e-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index         0         1             2\n",
              "0   26457  0.024023  0.063343  9.126337e-01\n",
              "1   26458  0.133268  0.135334  7.313976e-01\n",
              "2   26459  0.203205  0.193600  6.031955e-01\n",
              "3   26460  0.153787  0.173574  6.726397e-01\n",
              "4   26461  0.186106  0.141842  6.720521e-01\n",
              "5   26462  0.001327  0.554976  4.436972e-01\n",
              "6   26463  0.306671  0.685901  7.427885e-03\n",
              "7   26464  0.171500  0.207762  6.207376e-01\n",
              "8   26465  0.097556  0.134409  7.680354e-01\n",
              "9   26466  0.131565  0.169883  6.985524e-01\n",
              "10  26467  0.141042  0.151706  7.072527e-01\n",
              "11  26468  0.090468  0.172488  7.370440e-01\n",
              "12  26469  0.999525  0.000475  1.332234e-11\n",
              "13  26470  0.124237  0.145297  7.304663e-01\n",
              "14  26471  0.113739  0.139498  7.467627e-01\n",
              "15  26472  0.096252  0.182203  7.215447e-01\n",
              "16  26473  0.216466  0.246271  5.372629e-01\n",
              "17  26474  0.296656  0.683857  1.948757e-02\n",
              "18  26475  0.325381  0.275219  3.994006e-01\n",
              "19  26476  0.016471  0.039693  9.438356e-01"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 761
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytw_oizaXndc"
      },
      "source": [
        "submission.to_csv(PATH+'/result/wide-deep4.csv',index=False)"
      ],
      "execution_count": 762,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeBsXRMRX2J5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}