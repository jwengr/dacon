{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1JO3_viqnpLVEKeXwJO3javJaOCLyXwl7",
      "authorship_tag": "ABX9TyNK13zLhohNDl0bNX1dsox2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43d7c8064c0d42fe903490a1f5d94549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_31120dc4eba445a2abdd26b83db9235a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5041964700214458b5ea93920a3ea29d",
              "IPY_MODEL_d11981d1d13a4d31bd7f66c2f6867af8"
            ]
          }
        },
        "31120dc4eba445a2abdd26b83db9235a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5041964700214458b5ea93920a3ea29d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c959a1dd62ff46bab8e2ae4daa4f26a9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52bb3e4cec1c4ac0a3f14d1477a3d2d8"
          }
        },
        "d11981d1d13a4d31bd7f66c2f6867af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f0fcd06c18f34b919989a6a5635f3fa7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:02&lt;00:00, 369kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22692ed06dbe49fdbf3e523d984faf4b"
          }
        },
        "c959a1dd62ff46bab8e2ae4daa4f26a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52bb3e4cec1c4ac0a3f14d1477a3d2d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0fcd06c18f34b919989a6a5635f3fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22692ed06dbe49fdbf3e523d984faf4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwengr/dacon/blob/main/%EC%86%8C%EC%84%A4%20%EC%9E%91%EA%B0%80%20%EB%B6%84%EB%A5%98%20AI%20%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Vb3W2g94tt"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import re\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.externals import joblib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9VIfT8i-ldT"
      },
      "source": [
        "!pip install torchcontrib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L7aKRQ6Bn13"
      },
      "source": [
        "import torch.nn\r\n",
        "from torch.nn import CrossEntropyLoss\r\n",
        "import torch.nn.functional\r\n",
        "from torch.nn.functional import softmax\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchcontrib.optim import SWA\r\n",
        "from torch.utils.data import TensorDataset, random_split\r\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP4b2-s4B57r"
      },
      "source": [
        "import torch\r\n",
        "if torch.cuda.is_available():     \r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSpEavnqHNyj"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuJOFI3rHbZ7"
      },
      "source": [
        "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetConfig, XLNetForSequenceClassification\r\n",
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyFAixmJ0XWy"
      },
      "source": [
        "defaultpath = 'drive/My Drive/dacon/sosul/dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV_8Td1H6LQt"
      },
      "source": [
        "기본전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pZm5swI0nrl"
      },
      "source": [
        "train_df = pd.read_csv(defaultpath+'/train.csv',encoding='utf-8')\r\n",
        "train_df = train_df[train_df['text'].str.contains('\\* \\*')==False]\r\n",
        "train_df['sentencelen'] = train_df['text'].apply(lambda x: len(x.split('.')))\r\n",
        "train_df['charlen'] = train_df['text'].apply(lambda x: len(x))\r\n",
        "train_df['c/s'] = train_df['charlen']/(train_df['sentencelen']+1)  ## 0으로 나뉘는것을 방지\r\n",
        "train_df['upperlen'] = train_df['text'].apply(lambda x: len(re.findall('[A-Z]',x)))\r\n",
        "train_df['u/s'] = train_df['upperlen']/(train_df['sentencelen']+1)  ## 0으로 나뉘는것을 방지\r\n",
        "train_df['u/s'] = train_df['upperlen']/(train_df['charlen']+1)  ## 0으로 나뉘는것을 방지\r\n",
        "\r\n",
        "train_df_fr = train_df[train_df['text'].str.contains('[à|ä|ö|î|ù|â|Œ|ç|ê|ü|ñ|ô|Æ|œ|ë|æ|é|Ê|è|ì]')].copy()\r\n",
        "\r\n",
        "train, valid = train_test_split(train_df,test_size=0.2, random_state=2021, stratify=train_df['author']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CybgJhvMnH9c"
      },
      "source": [
        "tfidf all feature + mlp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4qJKvJHnMVw"
      },
      "source": [
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-zA-Z]+|\\W\",lowercase=True,dtype=np.float32).fit(train_df['text'])\r\n",
        "\r\n",
        "class TfidfDataset(Dataset):\r\n",
        "    def __init__(self,tfidfv=None,df=None):\r\n",
        "        self.tfidfv = tfidfv\r\n",
        "        self.df = df\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.df)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        df = self.df.iloc[idx,:]\r\n",
        "        enc = self.tfidfv.transform([df['text']]).toarray().astype(np.float32)\r\n",
        "        enc = torch.from_numpy(enc[0]).tolist()\r\n",
        "        df = df.drop(['index','text','sentencelen','charlen','upperlen'])\r\n",
        "        add = torch.from_numpy(df.drop('author').values.astype(np.float32)).tolist()\r\n",
        "        input_ids = enc+add\r\n",
        "        labels = df['author'].astype(np.int32).tolist()\r\n",
        "        \r\n",
        "        return input_ids,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYCtLMGc1zVU"
      },
      "source": [
        "tfidf_train_dataset = TfidfDataset(tfidfv,train)\r\n",
        "tfidf_valid_dataset = TfidfDataset(tfidfv,valid)\r\n",
        "def collate_fn(batch):\r\n",
        "    return list(zip(*batch))\r\n",
        "tfidf_train_dataloader = DataLoader(tfidf_train_dataset, batch_size=4, shuffle=True, num_workers=2,collate_fn=collate_fn)\r\n",
        "tfidf_valid_dataloader = DataLoader(tfidf_valid_dataset, batch_size=4, shuffle=True, num_workers=2,collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYm-ClNARF3M",
        "outputId": "4a245ca2-0dfa-4f69-db22-b0afac081a66"
      },
      "source": [
        "len(i[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33687"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLp_Y1hsnYl1"
      },
      "source": [
        "class TfidfMLPModel(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(TfidfMLPModel, self).__init__()\r\n",
        "        self.linear1 = torch.nn.Linear(33687,4096)\r\n",
        "        self.linear2 = torch.nn.Linear(4096,512)\r\n",
        "        self.linear3 = torch.nn.Linear(512,64)\r\n",
        "        self.linear4 = torch.nn.Linear(64,8)\r\n",
        "        self.linear5 = torch.nn.Linear(8,5)\r\n",
        "        \r\n",
        "        self.drop1 = torch.nn.Dropout()\r\n",
        "        self.drop2 = torch.nn.Dropout()\r\n",
        "        self.drop3 = torch.nn.Dropout()\r\n",
        "\r\n",
        "        torch.nn.init.xavier_normal_(self.linear1.weight)\r\n",
        "        torch.nn.init.xavier_normal_(self.linear2.weight)\r\n",
        "        torch.nn.init.xavier_normal_(self.linear3.weight)\r\n",
        "        torch.nn.init.xavier_normal_(self.linear4.weight)\r\n",
        "        torch.nn.init.xavier_normal_(self.linear5.weight)\r\n",
        "        \r\n",
        "    def forward(self, input_ids):\r\n",
        "        x = self.linear1(input_ids)\r\n",
        "        x = self.drop1(x)\r\n",
        "        x = self.linear2(x)\r\n",
        "        x = self.drop2(x)\r\n",
        "        x = self.linear3(x)\r\n",
        "        x = self.drop3(x)\r\n",
        "        x = self.linear4(x)\r\n",
        "        logits = self.linear5(x)\r\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci-HxQThqIJJ"
      },
      "source": [
        "# function to save and load the model form a specific epoch\r\n",
        "def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist,train_acc_hist,valid_acc_hist):\r\n",
        "\r\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\r\n",
        "    checkpoint = {'epochs': epochs, \r\n",
        "                    'lowest_eval_loss': lowest_eval_loss,\r\n",
        "                    'state_dict': model_to_save.state_dict(),\r\n",
        "                    'train_loss_hist': train_loss_hist,\r\n",
        "                    'valid_loss_hist': valid_loss_hist,\r\n",
        "                    'train_acc_hist' : train_acc_hist,\r\n",
        "                    'valid_acc_hist' : valid_acc_hist\r\n",
        "                }\r\n",
        "    torch.save(checkpoint, save_path+'/MLP_e{0}_loss{1:04.4f}_acc{2:04.4f}.pth'.format(epochs,lowest_eval_loss,valid_acc_hist[-1]))\r\n",
        "    print(\"Saving model at epoch {0} with validation loss of {1} vaildation acc of {2}\".format(epochs,\r\n",
        "                                                                        lowest_eval_loss,valid_acc_hist[-1]))\r\n",
        "    return\r\n",
        "  \r\n",
        "def load_model(save_path):\r\n",
        "    checkpoint = torch.load(save_path)\r\n",
        "    model_state_dict = checkpoint['state_dict']\r\n",
        "    model = TfidfMLPModel()\r\n",
        "    model.load_state_dict(model_state_dict)    \r\n",
        "    return model, checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tC19LSiqPNu"
      },
      "source": [
        "model = TfidfMLPModel()\r\n",
        "#model, checkpoint = load_model(defaultpath+'/model/MLP_e54_loss0.1661_acc0.7747.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tECuegYTqk0F"
      },
      "source": [
        "adamOptimizer = AdamW(model.parameters(),lr = 1e-5, eps = 1e-8, correct_bias=False)\r\n",
        "optimizer = SWA(adamOptimizer, swa_start=4, swa_freq=3, swa_lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JV-EnUIqo1q"
      },
      "source": [
        "def model_train(model, num_epochs,optimizer,\r\n",
        "          train_dataloader, valid_dataloader,model_save_path,checkpoint,device=\"cpu\"\r\n",
        "          ):\r\n",
        "    if checkpoint is None:\r\n",
        "        start_epoch=0\r\n",
        "        lowest_eval_loss = float('inf')\r\n",
        "        train_loss_hist = []\r\n",
        "        valid_loss_hist = []\r\n",
        "        train_acc_hist = []\r\n",
        "        valid_acc_hist = []\r\n",
        "    else:\r\n",
        "        start_epoch = checkpoint[\"epochs\"]+1\r\n",
        "        lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\r\n",
        "        train_loss_hist = checkpoint[\"train_loss_hist\"]\r\n",
        "        valid_loss_hist = checkpoint[\"valid_loss_hist\"]\r\n",
        "        train_acc_hist = checkpoint[\"train_acc_hist\"]\r\n",
        "        valid_acc_hist = checkpoint[\"valid_acc_hist\"]\r\n",
        "\r\n",
        "    model.to(device)\r\n",
        "    for i in range(start_epoch,num_epochs):\r\n",
        "        actual_epoch = i\r\n",
        "\r\n",
        "        model.train()\r\n",
        "        tr_acc = 0\r\n",
        "        tr_loss = 0\r\n",
        "        num_train_samples = 0\r\n",
        "        train_bar = tqdm(train_dataloader,desc=f\"Epoch {actual_epoch} Train \")\r\n",
        "        for step, batch in enumerate(train_bar):\r\n",
        "            b_input_ids, b_labels = torch.FloatTensor(batch[0]).to(device), torch.LongTensor(batch[1]).to(device)\r\n",
        "            num_train_samples += b_labels.size(0) \r\n",
        "\r\n",
        "            optimizer.zero_grad()\r\n",
        "            logits = model(input_ids=b_input_ids)\r\n",
        "            loss = CrossEntropyLoss()(logits, b_labels)\r\n",
        "\r\n",
        "            prediction = logits.data.max(1)[1]\r\n",
        "            tr_acc += prediction.eq(b_labels.data).sum().item()\r\n",
        "            tr_loss += loss.item()\r\n",
        "            train_bar.set_postfix({'train_acc': tr_acc/num_train_samples,'train_loss':tr_loss/num_train_samples})\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "        optimizer.swap_swa_sgd()\r\n",
        "        train_loss_hist.append(tr_loss/num_train_samples)\r\n",
        "        train_acc_hist.append(tr_acc/num_train_samples)\r\n",
        "            \r\n",
        "        model.eval()\r\n",
        "        eval_loss = 0\r\n",
        "        eval_acc = 0\r\n",
        "        num_eval_samples = 0\r\n",
        "        with torch.no_grad():\r\n",
        "            valid_bar = tqdm(valid_dataloader,desc=f\"Epoch {actual_epoch} Valid \")\r\n",
        "            for batch in valid_bar:\r\n",
        "                b_input_ids, b_labels = torch.FloatTensor(batch[0]).to(device), torch.LongTensor(batch[1]).to(device)\r\n",
        "\r\n",
        "                logits = model(input_ids=b_input_ids)\r\n",
        "                loss = CrossEntropyLoss()(logits, b_labels)\r\n",
        "                prediction = logits.data.max(1)[1]\r\n",
        "\r\n",
        "                eval_acc += prediction.eq(b_labels.data).sum().item()\r\n",
        "                eval_loss += loss.item()\r\n",
        "                num_eval_samples += b_labels.size(0)\r\n",
        "                valid_bar.set_postfix({'valid_acc':eval_acc/num_eval_samples,'valid_loss':eval_loss/num_eval_samples})\r\n",
        "\r\n",
        "            valid_loss_hist.append(eval_loss/num_eval_samples)\r\n",
        "            valid_acc_hist.append(eval_acc/num_eval_samples)\r\n",
        "            \r\n",
        "        if valid_loss_hist[-1] < lowest_eval_loss:\r\n",
        "            lowest_eval_loss = valid_loss_hist[-1]\r\n",
        "            save_model(model, model_save_path, actual_epoch, lowest_eval_loss, train_loss_hist, valid_loss_hist,train_acc_hist,valid_acc_hist)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKENV-5Hq6Ul"
      },
      "source": [
        "model = model_train(model=model, num_epochs = 100, \r\n",
        "            model_save_path=defaultpath+'/model', checkpoint=None,\r\n",
        "            optimizer=optimizer, device=device,\r\n",
        "           train_dataloader=tfidf_train_dataloader, valid_dataloader=tfidf_valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amgJUGt0nDB0"
      },
      "source": [
        "MLP_e27_loss0.1475_acc0.8033.pth saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JP-BSfgTEuF"
      },
      "source": [
        "길이가 매우 긴 문장이 많이때문에 XLNet을 이용하기로 결정하였습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s91dQv4j4-ux"
      },
      "source": [
        "pretrained xlnet tokenizer + unpretrained xlnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "43d7c8064c0d42fe903490a1f5d94549",
            "31120dc4eba445a2abdd26b83db9235a",
            "5041964700214458b5ea93920a3ea29d",
            "d11981d1d13a4d31bd7f66c2f6867af8",
            "c959a1dd62ff46bab8e2ae4daa4f26a9",
            "52bb3e4cec1c4ac0a3f14d1477a3d2d8",
            "f0fcd06c18f34b919989a6a5635f3fa7",
            "22692ed06dbe49fdbf3e523d984faf4b"
          ]
        },
        "id": "63G8wzmgn2iC",
        "outputId": "b688d3c9-1a20-4b65-cd8a-a91e6559b3f5"
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "input_ids = pd.DataFrame(train_df['text'].apply(lambda x: tokenizer.encode(x)).tolist()).fillna(0).astype(np.int)\n",
        "train_input_ids, valid_input_ids = train_test_split(input_ids,test_size=0.2, random_state=2021, stratify=train_df['author']) \n",
        "train_attention_masks = (train_input_ids>0).astype(np.int).values\n",
        "valid_attention_masks = (valid_input_ids>0).astype(np.int).values\n",
        "train_input_ids = torch.from_numpy(train_input_ids.values)\n",
        "valid_input_ids = torch.from_numpy(valid_input_ids.values)\n",
        "train_attention_masks = torch.from_numpy(train_attention_masks)\n",
        "valid_attention_masks = torch.from_numpy(valid_attention_masks)\n",
        "train_labels, valid_labels = train_test_split(train_df['author'].astype(np.int),test_size=0.2, random_state=2021, stratify=train_df['author']) \n",
        "train_labels = torch.from_numpy(train_labels.values)\n",
        "valid_labels = torch.from_numpy(valid_labels.values)\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43d7c8064c0d42fe903490a1f5d94549",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjz17fKi6hlV"
      },
      "source": [
        "batch_size = 8\r\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset),batch_size = batch_size)\r\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler = SequentialSampler(valid_dataset), batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvY5E3dJ7x5J"
      },
      "source": [
        "config = XLNetConfig(\r\n",
        "    vocab_size= tokenizer.vocab_size,\r\n",
        "    d_model= 32,\r\n",
        "    n_layer= 8,\r\n",
        "    n_head=16,\r\n",
        "    d_inner=128\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQV3L3ekBz1_"
      },
      "source": [
        "class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\r\n",
        "  \r\n",
        "    def __init__(self,config):\r\n",
        "        super(XLNetForMultiLabelSequenceClassification, self).__init__()\r\n",
        "        self.xlnet = XLNetModel(config)\r\n",
        "        self.linear = torch.nn.Linear(32, 5)\r\n",
        "\r\n",
        "        torch.nn.init.xavier_normal_(self.linear.weight)\r\n",
        "\r\n",
        "    def forward(self, input_ids, token_type_ids=None,\r\n",
        "                attention_mask=None):\r\n",
        "\r\n",
        "        last_hidden_state = self.xlnet(input_ids=input_ids,\r\n",
        "                                    attention_mask=attention_mask,\r\n",
        "                                    token_type_ids=token_type_ids\r\n",
        "                                    )\r\n",
        "        mean_last_hidden_state = torch.mean(last_hidden_state[0],1)\r\n",
        "        logits = self.linear(mean_last_hidden_state)\r\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImqDBusCCD4J"
      },
      "source": [
        "# function to save and load the model form a specific epoch\r\n",
        "def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist,train_acc_hist,valid_acc_hist):\r\n",
        "\r\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\r\n",
        "    checkpoint = {'epochs': epochs, \r\n",
        "                    'lowest_eval_loss': lowest_eval_loss,\r\n",
        "                    'state_dict': model_to_save.state_dict(),\r\n",
        "                    'train_loss_hist': train_loss_hist,\r\n",
        "                    'valid_loss_hist': valid_loss_hist,\r\n",
        "                    'train_acc_hist' : train_acc_hist,\r\n",
        "                    'valid_acc_hist' : valid_acc_hist\r\n",
        "                }\r\n",
        "    torch.save(checkpoint, save_path+'/pretokenXLNET_e{0}_loss{1:04.4f}_acc{2:04.4f}.pth'.format(epochs,lowest_eval_loss,valid_acc_hist[-1]))\r\n",
        "    print(\"Saving model at epoch {0} with validation loss of {1} vaildation acc of {2}\".format(epochs,\r\n",
        "                                                                        lowest_eval_loss,valid_acc_hist[-1]))\r\n",
        "    return\r\n",
        "  \r\n",
        "def load_model(save_path):\r\n",
        "    checkpoint = torch.load(save_path)\r\n",
        "    model_state_dict = checkpoint['state_dict']\r\n",
        "    model = XLNetForMultiLabelSequenceClassification(config=config)\r\n",
        "    model.load_state_dict(model_state_dict)    \r\n",
        "    return model, checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9JWE8mj87k9"
      },
      "source": [
        "#model = XLNetForMultiLabelSequenceClassification(config=config)\r\n",
        "model, checkpoint = load_model(defaultpath+'/model/pretokenXLNET_e44_loss0.0806_acc0.7938.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qasGlO6gkdB6"
      },
      "source": [
        "Stochastic Weight Averaging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLV93wQG8-08"
      },
      "source": [
        "adamOptimizer = AdamW(model.parameters(),lr = 1e-5, eps = 1e-8, correct_bias=False)\r\n",
        "optimizer = SWA(adamOptimizer, swa_start=4, swa_freq=3, swa_lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtcAWvL0_XxW"
      },
      "source": [
        "def model_train(model, num_epochs,optimizer,\r\n",
        "          train_dataloader, valid_dataloader,model_save_path,checkpoint,device=\"cpu\"\r\n",
        "          ):\r\n",
        "    if checkpoint is None:\r\n",
        "        start_epoch=0\r\n",
        "        lowest_eval_loss = float('inf')\r\n",
        "        train_loss_hist = []\r\n",
        "        valid_loss_hist = []\r\n",
        "        train_acc_hist = []\r\n",
        "        valid_acc_hist = []\r\n",
        "    else:\r\n",
        "        start_epoch = checkpoint[\"epochs\"]+1\r\n",
        "        lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\r\n",
        "        train_loss_hist = checkpoint[\"train_loss_hist\"]\r\n",
        "        valid_loss_hist = checkpoint[\"valid_loss_hist\"]\r\n",
        "        train_acc_hist = checkpoint[\"train_acc_hist\"]\r\n",
        "        valid_acc_hist = checkpoint[\"valid_acc_hist\"]\r\n",
        "\r\n",
        "    model.to(device)\r\n",
        "    for i in range(start_epoch,num_epochs):\r\n",
        "        actual_epoch = i\r\n",
        "\r\n",
        "        model.train()\r\n",
        "        tr_acc = 0\r\n",
        "        tr_loss = 0\r\n",
        "        num_train_samples = 0\r\n",
        "        train_bar = tqdm(train_dataloader,desc=f\"Epoch {actual_epoch} Train \")\r\n",
        "        for step, batch in enumerate(train_bar):\r\n",
        "            b_input_ids, b_attn_masks ,b_labels = (b.long().to(device) for b in batch)\r\n",
        "            num_train_samples += b_labels.size(0) \r\n",
        "\r\n",
        "            optimizer.zero_grad()\r\n",
        "            logits = model(input_ids=b_input_ids,attention_mask=b_attn_masks)\r\n",
        "            loss = CrossEntropyLoss()(logits, b_labels)\r\n",
        "\r\n",
        "            prediction = logits.data.max(1)[1]\r\n",
        "            tr_acc += prediction.eq(b_labels.data).sum().item()\r\n",
        "            tr_loss += loss.item()\r\n",
        "            train_bar.set_postfix({'train_acc': tr_acc/num_train_samples,'train_loss':tr_loss/num_train_samples})\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "        optimizer.swap_swa_sgd()\r\n",
        "        train_loss_hist.append(tr_loss/num_train_samples)\r\n",
        "        train_acc_hist.append(tr_acc/num_train_samples)\r\n",
        "            \r\n",
        "        model.eval()\r\n",
        "        eval_loss = 0\r\n",
        "        eval_acc = 0\r\n",
        "        num_eval_samples = 0\r\n",
        "        with torch.no_grad():\r\n",
        "            valid_bar = tqdm(valid_dataloader,desc=f\"Epoch {actual_epoch} Valid \")\r\n",
        "            for batch in valid_bar:\r\n",
        "                b_input_ids, b_attn_masks ,b_labels = (b.long().to(device) for b in batch)\r\n",
        "\r\n",
        "                logits = model(input_ids=b_input_ids,attention_mask=b_attn_masks)\r\n",
        "                loss = CrossEntropyLoss()(logits, b_labels)\r\n",
        "                prediction = logits.data.max(1)[1]\r\n",
        "\r\n",
        "                eval_acc += prediction.eq(b_labels.data).sum().item()\r\n",
        "                eval_loss += loss.item()\r\n",
        "                num_eval_samples += b_labels.size(0)\r\n",
        "                valid_bar.set_postfix({'valid_acc':eval_acc/num_eval_samples,'valid_loss':eval_loss/num_eval_samples})\r\n",
        "\r\n",
        "            valid_loss_hist.append(eval_loss/num_eval_samples)\r\n",
        "            valid_acc_hist.append(eval_acc/num_eval_samples)\r\n",
        "            \r\n",
        "        if valid_loss_hist[-1] < lowest_eval_loss:\r\n",
        "            lowest_eval_loss = valid_loss_hist[-1]\r\n",
        "            save_model(model, model_save_path, actual_epoch, lowest_eval_loss, train_loss_hist, valid_loss_hist,train_acc_hist,valid_acc_hist)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SStcZOpNr5H"
      },
      "source": [
        "model = model_train(model=model, num_epochs = 100, \r\n",
        "            model_save_path=defaultpath+'/model', checkpoint=checkpoint,\r\n",
        "            optimizer=optimizer, device=device,\r\n",
        "           train_dataloader=train_dataloader, valid_dataloader=valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btyy-KSei8y8"
      },
      "source": [
        "pretokenXLNET_e47_loss0.0804_acc0.7946.pth saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkDe3gH3HpFl"
      },
      "source": [
        "SentencePiece + unpretrained XLNET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlzJciEswR7w"
      },
      "source": [
        "with open(defaultpath+'/train.txt', 'w', encoding='utf8') as f:\r\n",
        "    f.write('\\n'.join(train_df['text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyfJggElEmkx"
      },
      "source": [
        "spm.SentencePieceTrainer.train(input=defaultpath+'/train.txt',vocab_size=32000,\r\n",
        "                               model_prefix=f'{defaultpath}/spm32000',character_coverage=1.0,\r\n",
        "                               model_type='bpe', accept_language=['en','fr'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WoXRbiLD-lO",
        "outputId": "caa51940-6cbf-4150-fee5-13099c2aed0c"
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\r\n",
        "vocab_file = defaultpath+\"/spm32000.model\"\r\n",
        "sp.load(vocab_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcyJClw1Fzfj"
      },
      "source": [
        "tokenizer = XLNetTokenizer(vocab_file,keep_accents=True,do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r76b2trtF8_9"
      },
      "source": [
        "input_ids = pd.DataFrame(train_df['text'].apply(lambda x: tokenizer.encode(x)).tolist()).fillna(0).astype(np.int)\r\n",
        "train_input_ids, valid_input_ids = train_test_split(input_ids,test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "train_attention_masks = (train_input_ids>0).astype(np.int).values\r\n",
        "valid_attention_masks = (valid_input_ids>0).astype(np.int).values\r\n",
        "train_input_ids = torch.from_numpy(train_input_ids.values)\r\n",
        "valid_input_ids = torch.from_numpy(valid_input_ids.values)\r\n",
        "train_attention_masks = torch.from_numpy(train_attention_masks)\r\n",
        "valid_attention_masks = torch.from_numpy(valid_attention_masks)\r\n",
        "train_labels, valid_labels = train_test_split(train_df['author'].astype(np.int),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "train_labels = torch.from_numpy(train_labels.values)\r\n",
        "valid_labels = torch.from_numpy(valid_labels.values)\r\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\r\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Loha_5GCN7"
      },
      "source": [
        "batch_size = 8\r\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset),batch_size = batch_size)\r\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler = SequentialSampler(valid_dataset), batch_size = batch_size)\r\n",
        "config = XLNetConfig(\r\n",
        "    vocab_size= tokenizer.vocab_size,\r\n",
        "    d_model= 32,\r\n",
        "    n_layer= 8,\r\n",
        "    n_head=16,\r\n",
        "    d_inner=128\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX2wQJBrGLTR"
      },
      "source": [
        "class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\r\n",
        "  \r\n",
        "    def __init__(self,config):\r\n",
        "        super(XLNetForMultiLabelSequenceClassification, self).__init__()\r\n",
        "        self.xlnet = XLNetModel(config)\r\n",
        "        self.linear = torch.nn.Linear(32, 5)\r\n",
        "\r\n",
        "        torch.nn.init.xavier_normal_(self.linear.weight)\r\n",
        "\r\n",
        "    def forward(self, input_ids, token_type_ids=None,\r\n",
        "                attention_mask=None):\r\n",
        "\r\n",
        "        last_hidden_state = self.xlnet(input_ids=input_ids,\r\n",
        "                                    attention_mask=attention_mask,\r\n",
        "                                    token_type_ids=token_type_ids\r\n",
        "                                    )\r\n",
        "        mean_last_hidden_state = torch.mean(last_hidden_state[0],1)\r\n",
        "        logits = self.linear(mean_last_hidden_state)\r\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxug8cg2GOvV"
      },
      "source": [
        "# function to save and load the model form a specific epoch\r\n",
        "def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist,train_acc_hist,valid_acc_hist):\r\n",
        "\r\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\r\n",
        "    checkpoint = {'epochs': epochs, \r\n",
        "                    'lowest_eval_loss': lowest_eval_loss,\r\n",
        "                    'state_dict': model_to_save.state_dict(),\r\n",
        "                    'train_loss_hist': train_loss_hist,\r\n",
        "                    'valid_loss_hist': valid_loss_hist,\r\n",
        "                    'train_acc_hist' : train_acc_hist,\r\n",
        "                    'valid_acc_hist' : valid_acc_hist\r\n",
        "                }\r\n",
        "    torch.save(checkpoint, save_path+'/vanillaXLNET_e{0}_loss{1:04.4f}_acc{2:04.4f}.pth'.format(epochs,lowest_eval_loss,valid_acc_hist[-1]))\r\n",
        "    print(\"Saving model at epoch {0} with validation loss of {1} vaildation acc of {2}\".format(epochs,\r\n",
        "                                                                        lowest_eval_loss,valid_acc_hist[-1]))\r\n",
        "    return\r\n",
        "  \r\n",
        "def load_model(save_path):\r\n",
        "    checkpoint = torch.load(save_path)\r\n",
        "    model_state_dict = checkpoint['state_dict']\r\n",
        "    model = XLNetForMultiLabelSequenceClassification(config=config)\r\n",
        "    model.load_state_dict(model_state_dict)    \r\n",
        "    return model, checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tapxj4x5GRko"
      },
      "source": [
        "#model, checkpoint = XLNetForMultiLabelSequenceClassification(config=config), None\r\n",
        "model, checkpoint = load_model(defaultpath+'/model/vanillaXLNET_e42_loss0.0802_acc0.8026.pth')\r\n",
        "adamOptimizer = AdamW(model.parameters(),lr = 1e-5, eps = 1e-8, correct_bias=False)\r\n",
        "optimizer = SWA(adamOptimizer, swa_start=4, swa_freq=3, swa_lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDERuCPPGdUI"
      },
      "source": [
        "def model_train(model, num_epochs,optimizer,\r\n",
        "          train_dataloader, valid_dataloader,model_save_path,checkpoint,device=\"cpu\"\r\n",
        "          ):\r\n",
        "    if checkpoint is None:\r\n",
        "        start_epoch=0\r\n",
        "        lowest_eval_loss = float('inf')\r\n",
        "        train_loss_hist = []\r\n",
        "        valid_loss_hist = []\r\n",
        "        train_acc_hist = []\r\n",
        "        valid_acc_hist = []\r\n",
        "    else:\r\n",
        "        start_epoch = checkpoint[\"epochs\"]+1\r\n",
        "        lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\r\n",
        "        train_loss_hist = checkpoint[\"train_loss_hist\"]\r\n",
        "        valid_loss_hist = checkpoint[\"valid_loss_hist\"]\r\n",
        "        train_acc_hist = checkpoint[\"train_acc_hist\"]\r\n",
        "        valid_acc_hist = checkpoint[\"valid_acc_hist\"]\r\n",
        "\r\n",
        "    model.to(device)\r\n",
        "    for i in range(start_epoch,num_epochs):\r\n",
        "        actual_epoch = i\r\n",
        "\r\n",
        "        model.train()\r\n",
        "        tr_acc = 0\r\n",
        "        tr_loss = 0\r\n",
        "        num_train_samples = 0\r\n",
        "        train_bar = tqdm(train_dataloader,desc=f\"Epoch {actual_epoch} Train \")\r\n",
        "        for step, batch in enumerate(train_bar):\r\n",
        "            b_input_ids, b_attn_masks ,b_labels = (b.long().to(device) for b in batch)\r\n",
        "            num_train_samples += b_labels.size(0) \r\n",
        "\r\n",
        "            optimizer.zero_grad()\r\n",
        "            logits = model(input_ids=b_input_ids,attention_mask=b_attn_masks)\r\n",
        "            loss = CrossEntropyLoss()(logits, b_labels)\r\n",
        "\r\n",
        "            prediction = logits.data.max(1)[1]\r\n",
        "            tr_acc += prediction.eq(b_labels.data).sum().item()\r\n",
        "            tr_loss += loss.item()\r\n",
        "            train_bar.set_postfix({'train_acc': tr_acc/num_train_samples,'train_loss':tr_loss/num_train_samples})\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "        optimizer.swap_swa_sgd()\r\n",
        "        train_loss_hist.append(tr_loss/num_train_samples)\r\n",
        "        train_acc_hist.append(tr_acc/num_train_samples)\r\n",
        "            \r\n",
        "        model.eval()\r\n",
        "        eval_loss = 0\r\n",
        "        eval_acc = 0\r\n",
        "        num_eval_samples = 0\r\n",
        "        with torch.no_grad():\r\n",
        "            valid_bar = tqdm(valid_dataloader,desc=f\"Epoch {actual_epoch} Valid \")\r\n",
        "            for batch in valid_bar:\r\n",
        "                b_input_ids, b_attn_masks ,b_labels = (b.long().to(device) for b in batch)\r\n",
        "\r\n",
        "                logits = model(input_ids=b_input_ids,attention_mask=b_attn_masks)\r\n",
        "                loss = CrossEntropyLoss()(logits, b_labels)\r\n",
        "                prediction = logits.data.max(1)[1]\r\n",
        "\r\n",
        "                eval_acc += prediction.eq(b_labels.data).sum().item()\r\n",
        "                eval_loss += loss.item()\r\n",
        "                num_eval_samples += b_labels.size(0)\r\n",
        "                valid_bar.set_postfix({'valid_acc':eval_acc/num_eval_samples,'valid_loss':eval_loss/num_eval_samples})\r\n",
        "\r\n",
        "            valid_loss_hist.append(eval_loss/num_eval_samples)\r\n",
        "            valid_acc_hist.append(eval_acc/num_eval_samples)\r\n",
        "            \r\n",
        "        if valid_loss_hist[-1] < lowest_eval_loss:\r\n",
        "            lowest_eval_loss = valid_loss_hist[-1]\r\n",
        "            save_model(model, model_save_path, actual_epoch, lowest_eval_loss, train_loss_hist, valid_loss_hist,train_acc_hist,valid_acc_hist)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lX_U44TGglF"
      },
      "source": [
        "model = model_train(model=model, num_epochs = 100, \r\n",
        "            model_save_path=defaultpath+'/model', checkpoint=checkpoint,\r\n",
        "            optimizer=optimizer, device=device,\r\n",
        "           train_dataloader=train_dataloader, valid_dataloader=valid_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pZnK3OjovcF"
      },
      "source": [
        "vanillaXLNET_e44_loss0.0798_acc0.8052.pth saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HERySBKtGkH2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}