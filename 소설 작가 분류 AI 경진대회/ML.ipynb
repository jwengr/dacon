{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1escBmOE3uPg2keHE49kAxu1-fUuatsaT",
      "authorship_tag": "ABX9TyPjOwudj28dQ3ksG5zY1nLb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwengr/dacon/blob/main/%EC%86%8C%EC%84%A4%20%EC%9E%91%EA%B0%80%20%EB%B6%84%EB%A5%98%20AI%20%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HrpV-gvxeKA"
      },
      "source": [
        "EDA에서 특수문자의 사용빈도나 프랑스문자의 사용빈도가 결과 예측에\r\n",
        "도움이 된 다는 것을 알 수 있었습니다.\r\n",
        "일반적인 딥러닝에서의 텍스트분석은 주로 특수문자들을 없애거나 최소화하여\r\n",
        "문장의 뜻을 맞춥니다.\r\n",
        "하지만 우리는 주어진 텍스트에서 작가를 분류해야하며\r\n",
        "주어진 텍스트는 서로 비슷합니다.\r\n",
        "즉 텍스트간의 미묘한 차이를 구분하는 모델을 만들어야 하며, 특수문자나 프랑스문자에도 집중해야 할 것 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taCgB-3xmzRA"
      },
      "source": [
        "!pip uninstall -y lightgbm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztsJLKoL9lOE"
      },
      "source": [
        "!git clone --recursive https://github.com/Microsoft/LightGBM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXDuq7OSAdUW"
      },
      "source": [
        "!cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Wpfdyu4GNv"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eleGxfBzykhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d184a09-77fe-4774-e96a-edbd6907d52a"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import xgboost as xgb\r\n",
        "import catboost as ctb\r\n",
        "import lightgbm as lgb\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.externals import joblib"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u30611Qb1LpD"
      },
      "source": [
        "defaultpath = 'drive/My Drive/dacon/sosul/dataset'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ9s3j1L23px"
      },
      "source": [
        "train_df = pd.read_csv(defaultpath+'/train.csv',encoding='utf-8')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2A2Y1y_25Op"
      },
      "source": [
        "기본 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz1m9aJuKsPx"
      },
      "source": [
        "train_df = train_df[train_df['text'].str.contains('\\* \\*')==False]\n",
        "train_df['sentencelen'] = train_df['text'].apply(lambda x: len(x.split('.')))\n",
        "train_df['charlen'] = train_df['text'].apply(lambda x: len(x))\n",
        "train_df['c/s'] = train_df['charlen']/(train_df['sentencelen']+1)  ## 0으로 나뉘는것을 방지\n",
        "train_df['upperlen'] = train_df['text'].apply(lambda x: len(re.findall('[A-Z]',x)))\n",
        "train_df['u/s'] = train_df['upperlen']/(train_df['sentencelen']+1)  ## 0으로 나뉘는것을 방지\n",
        "train_df['u/s'] = train_df['upperlen']/(train_df['charlen']+1)  ## 0으로 나뉘는것을 방지"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeLnICpYHZiB"
      },
      "source": [
        "프랑스어가 포함된 문장만 따로 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBhJYzwGHin1"
      },
      "source": [
        "train_df_fr = train_df[train_df['text'].str.contains('[à|ä|ö|î|ù|â|Œ|ç|ê|ü|ñ|ô|Æ|œ|ë|æ|é|Ê|è|ì]')].copy()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD0hT7Pi-jrH"
      },
      "source": [
        "char TF-IDF : 특수문자까지 포함하여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caA7spOmAlIv",
        "outputId": "b8b383af-fd9b-4317-de25-9c7f7423d8d2"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \n",
        "char_tfidfv  = TfidfVectorizer(analyzer='char').fit(train['text'])\n",
        "train_enc = char_tfidfv.transform(train['text']).toarray()\n",
        "test_enc = char_tfidfv.transform(test['text']).toarray()\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=10000)\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=10000,early_stopping_round=1000)\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=10000,early_stopping_rounds=1000,task_type=\"GPU\",loss_function='MultiClass')\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (1835, 0.5892222120908179)\n",
            "lgb : 0.5890398468131668\n",
            "ctb : (3695, 0.6025348773593507)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-as_BlJ-fN"
      },
      "source": [
        "char tfidf : 프랑스어포함문장만"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFs81Ch3D2VD",
        "outputId": "131244e3-54ef-43a0-edc2-b9cee45898f5"
      },
      "source": [
        "train, test = train_test_split(train_df_fr.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df_fr['author']) \r\n",
        "char_tfidfv  = TfidfVectorizer(analyzer='char').fit(train['text'])\r\n",
        "train_enc = char_tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = char_tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=10000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=10000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=10000,early_stopping_rounds=1000,task_type=\"GPU\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (72, 0.8507462686567164)\n",
            "lgb : 0.8805970149253731\n",
            "ctb : (1185, 0.8059701492537313)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIxh6mnVqCSl",
        "outputId": "6cc33ebd-b616-43db-e0a6-184ba722fe61"
      },
      "source": [
        "joblib.dump(lgb_model,defaultpath+'/model/fr_lgb_f5000.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/My Drive/dacon/sosul/dataset/model/fr_lgb_f5000.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcuGZsjHJvMb"
      },
      "source": [
        "TF-IDF : 특수문자까지 포함하여 features 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOy2yTbhEwGC",
        "outputId": "5557f13a-ade9-476b-91f6-8e76ee9b8d6f"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-zA-Z]+|\\W\",max_features= 500,lowercase=True).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=100000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=100000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=100000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (3621, 0.7249931613020881)\n",
            "lgb : 0.7318318592140056\n",
            "ctb : (42139, 0.7353879821282028)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTmZUi5_EsIc",
        "outputId": "f1fca1f4-3c01-4127-e54d-cfd966319268"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"(?=[A-Z]+)|[a-zA-Z]+|\\W\",max_features= 500,lowercase=False).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=100000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=100000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=100000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (3218, 0.7162396279748335)\n",
            "lgb : 0.7206163946384608\n",
            "ctb : (39492, 0.7297346585210176)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWaQ1CyhCVxH",
        "outputId": "8a87852b-1c8b-409f-bd49-f636f033475c"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"(?=[A-Z]+)|[a-zA-Z]+|\\W\",max_features= 500,lowercase=True).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=100000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=100000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=100000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (3621, 0.7249931613020881)\n",
            "lgb : 0.7318318592140056\n",
            "ctb : (42710, 0.7347497036564238)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDoWCgaqE-yz",
        "outputId": "abb4222a-fdeb-4f3d-8f3b-7b5401f0f677"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-z]+|[A-Z]+|\\W\",max_features= 500,lowercase=False).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=100000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=100000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=100000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (3470, 0.7189751071396007)\n",
            "lgb : 0.7267256314397739\n",
            "ctb : (40678, 0.7338378772681682)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5sIWkbUFHdp",
        "outputId": "384ba969-b7df-481f-a9ff-cf8b9624ae22"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-z]+|[A-Z]+|\\W\",max_features= 500,lowercase=True).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=100000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=100000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=100000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (3621, 0.7249931613020881)\n",
            "lgb : 0.7318318592140056\n",
            "ctb : (42159, 0.7352056168505516)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix8xwYoTFbDG",
        "outputId": "29c87d5e-ce25-457f-b8c9-039c77af0645"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-z]+|[A-Z]+|[0-9]+|\\W\",max_features= 500,lowercase=True).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=100000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=100000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=100000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (3621, 0.7249931613020881)\n",
            "lgb : 0.7318318592140056\n",
            "ctb : (43731, 0.7344761557399471)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1oNg1feFe8s",
        "outputId": "1874dc6a-a247-48b8-af8d-d52ca6128580"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-z]+|[A-Z]+|[0-9]+|\\W\",max_features= 500,lowercase=False).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=100000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=100000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=100000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (3470, 0.7189751071396007)\n",
            "lgb : 0.7267256314397739\n",
            "ctb : (41143, 0.7342937904622959)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O4ZhqslbYL5"
      },
      "source": [
        "소문자로 변환하여 토큰하는 것이 가장 높았습니다.\r\n",
        "다음은 불용어를 비교해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T31dJzH0bfVJ",
        "outputId": "e29a477c-755e-441c-833a-6971d44b3432"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-zA-Z]+|\\W\",max_features= 500,lowercase=True,stop_words='english').fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=100000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=100000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=100000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (3678, 0.6933527856296161)\n",
            "lgb : 0.6973648217379411\n",
            "ctb : (38432, 0.6973648217379411)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzudaT3AbX00"
      },
      "source": [
        "결과를 바탕으로 feature개수를 늘려보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZncIyoYqRrVI"
      },
      "source": [
        "TF-IDF : 특수문자까지 포함하여 features 1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfC7zB_Kq1r2",
        "outputId": "3da966c8-85ae-4ff1-ab1e-3a889a6d4760"
      },
      "source": [
        "train, test = train_test_split(train_df.drop('index',axis=1),test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-zA-Z]+|\\W\",max_features= 1000,lowercase=True).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray()\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray()\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=200000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=200000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=200000,early_stopping_rounds=1000,task_type=\"GPU\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (5306, 0.7466034467037476)\n",
            "lgb : 0.7523479529497583\n",
            "ctb : (93908, 0.7561776237804322)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz89C8HPM-vn"
      },
      "source": [
        "tfidf feature 2000\r\n",
        "float 64를 낮춰줍니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_0F3YsqJ1Qk",
        "outputId": "7245d00a-116a-4e13-ce8f-f824afa8deea"
      },
      "source": [
        "train, test = train_test_split(train_df,test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-zA-Z]+|\\W\",max_features= 2000,lowercase=True,dtype=np.float32).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray().astype(np.float16)\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray().astype(np.float16)\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['level_0','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['level_0','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=500000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=500000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=500000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (6358, 0.7640193307194311)\n",
            "lgb : 0.776055439044406\n",
            "ctb : (225452, 0.7732287772408134)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EIzsXH8InUN"
      },
      "source": [
        "featur 5000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKMunGsyB3d2",
        "outputId": "4ac389f2-cde5-45fd-a622-b5c1ef5a55df"
      },
      "source": [
        "train, test = train_test_split(train_df,test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-zA-Z]+|\\W\",max_features= 5000,lowercase=True,dtype=np.float32).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray().astype(np.float16)\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray().astype(np.float16)\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['level_0','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['level_0','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=500000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=500000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=500000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (9514, 0.7890945563964621)\n",
            "lgb : 0.794383149448345\n",
            "ctb : (348373, 0.7869973557034741)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_HaLJ4yq2L_"
      },
      "source": [
        "feature 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XitXfWRfs0Cl",
        "outputId": "1f089483-83f1-4428-8656-39d41c4ca770"
      },
      "source": [
        "train, test = train_test_split(train_df,test_size=0.2, random_state=2021, stratify=train_df['author']) \r\n",
        "tfidfv = TfidfVectorizer(token_pattern=\"[a-zA-Z]+|\\W\",max_features= 10000,lowercase=True,dtype=np.float32).fit(train_df['text'])\r\n",
        "train_enc = tfidfv.transform(train['text']).toarray().astype(np.float16)\r\n",
        "test_enc = tfidfv.transform(test['text']).toarray().astype(np.float16)\r\n",
        "train = pd.concat([train.reset_index(),pd.DataFrame(train_enc)],axis=1).drop(['level_0','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "test = pd.concat([test.reset_index(),pd.DataFrame(test_enc)],axis=1).drop(['level_0','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, y_train, x_test, y_test = train.drop('author',axis=1),train['author'], test.drop('author',axis=1),test['author']\r\n",
        "\r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=500000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=500000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=500000,early_stopping_rounds=1000,task_type=\"GPU\",devices=\"0:1\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "Warning: less than 75% gpu memory available for training. Free: 9564.875 Total: 16130.5\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (9380, 0.796297984863682)\n",
            "lgb : 0.7958420716695541\n",
            "ctb : (443818, 0.7880003647305553)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2TzIsbMs2YY",
        "outputId": "2210f72a-6477-42fe-9016-146bfd1910ae"
      },
      "source": [
        "joblib.dump(xgb_model,defaultpath+'/model/xgb_f10000.pkl')\r\n",
        "joblib.dump(lgb_model,defaultpath+'/model/lgb_f10000.pkl')\r\n",
        "joblib.dump(ctb_model,defaultpath+'/model/ctb_f10000.pkl')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/My Drive/dacon/sosul/dataset/model/ctb_f10000.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0N6E97lrsJ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}