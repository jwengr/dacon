{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1escBmOE3uPg2keHE49kAxu1-fUuatsaT",
      "authorship_tag": "ABX9TyN2t3fNStdR9VKzNLP49/jX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwengr/dacon/blob/main/%EC%86%8C%EC%84%A4%20%EC%9E%91%EA%B0%80%20%EB%B6%84%EB%A5%98%20AI%20%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HrpV-gvxeKA"
      },
      "source": [
        "EDA에서 특수문자의 사용빈도나 프랑스문자의 사용빈도가 결과 예측에\r\n",
        "도움이 된 다는 것을 알 수 있었습니다.\r\n",
        "일반적인 딥러닝에서의 텍스트분석은 주로 특수문자들을 없애거나 최소화하여\r\n",
        "문장의 뜻을 맞춥니다.\r\n",
        "하지만 우리는 주어진 텍스트에서 작가를 분류해야하며\r\n",
        "주어진 텍스트는 서로 비슷합니다.\r\n",
        "즉 텍스트간의 미묘한 차이를 구분하는 모델을 만들어야 하며, 특수문자나 프랑스문자에도 집중해야 할 것 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taCgB-3xmzRA"
      },
      "source": [
        "!pip uninstall lightgbm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztsJLKoL9lOE"
      },
      "source": [
        "!git clone --recursive https://github.com/Microsoft/LightGBM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXDuq7OSAdUW"
      },
      "source": [
        "!cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Wpfdyu4GNv"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eleGxfBzykhr"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import xgboost as xgb\r\n",
        "import catboost as ctb\r\n",
        "import lightgbm as lgb\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u30611Qb1LpD"
      },
      "source": [
        "defaultpath = 'drive/My Drive/dacon/sosul/dataset'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ9s3j1L23px"
      },
      "source": [
        "train_df = pd.read_csv(defaultpath+'/train.csv',encoding='utf-8')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2A2Y1y_25Op"
      },
      "source": [
        "기본 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz1m9aJuKsPx"
      },
      "source": [
        "train_df = train_df[train_df['text'].str.contains('\\* \\*')==False]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL41T-rpKxRB"
      },
      "source": [
        "train_df['sentencelen'] = train_df['text'].apply(lambda x: len(x.split('.')))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTIzDjzHMly9"
      },
      "source": [
        "train_df['charlen'] = train_df['text'].apply(lambda x: len(x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP1imdN-MnOO"
      },
      "source": [
        "train_df['c/s'] = train_df['charlen']/(train_df['sentencelen']+1)  ## 0으로 나뉘는것을 방지"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNUVPBz-Mo7Z"
      },
      "source": [
        "train_df['upperlen'] = train_df['text'].apply(lambda x: len(re.findall('[A-Z]',x)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQbawQcMsq7"
      },
      "source": [
        "train_df['u/s'] = train_df['upperlen']/(train_df['sentencelen']+1)  ## 0으로 나뉘는것을 방지"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOM2WodiMuJj"
      },
      "source": [
        "train_df['u/s'] = train_df['upperlen']/(train_df['charlen']+1)  ## 0으로 나뉘는것을 방지"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeLnICpYHZiB"
      },
      "source": [
        "프랑스어가 포함된 문장만 따로 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBhJYzwGHin1"
      },
      "source": [
        "train_df_fr = train_df[train_df['text'].str.contains('[à|ä|ö|î|ù|â|Œ|ç|ê|ü|ñ|ô|Æ|œ|ë|æ|é|Ê|è|ì]')].copy()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD0hT7Pi-jrH"
      },
      "source": [
        "char TF-IDF : 특수문자까지 포함하여"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giPFyFbi-tWG"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caA7spOmAlIv",
        "outputId": "73a0b35c-351a-46c4-b44a-3fa8a050d4f2"
      },
      "source": [
        "char_tfidf_train_df = train_df.copy()\n",
        "char_tfidfv  = TfidfVectorizer(analyzer='char').fit(char_tfidf_train_df['text'])\n",
        "enc = char_tfidfv.transform(char_tfidf_train_df['text']).toarray()\n",
        "char_tfidf_train_df = pd.concat([char_tfidf_train_df.reset_index(),pd.DataFrame(enc)],axis=1).drop(['level_0','index','text','sentencelen','charlen','upperlen'],axis=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(char_tfidf_train_df.drop('author',axis=1), char_tfidf_train_df['author'],\n",
        "                                                    test_size=0.2, random_state=2021,\n",
        "                                                    stratify=char_tfidf_train_df['author']) \n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=10000)\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=10000,early_stopping_round=1000)\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=10000,early_stopping_rounds=1000,task_type=\"GPU\",loss_function='MultiClass')\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (1908, 0.5873985593143065)\n",
            "lgb : 0.5929607002826662\n",
            "ctb : (3620, 0.6026260599981763)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-as_BlJ-fN"
      },
      "source": [
        "char tfidf : 프랑스어포함문장만"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFs81Ch3D2VD",
        "outputId": "ebcb9b41-9c17-496a-866a-66db1d9da9c3"
      },
      "source": [
        "char_tfidf_train_df = train_df_fr.copy()\r\n",
        "char_tfidfv  = TfidfVectorizer(analyzer='char').fit(char_tfidf_train_df['text'])\r\n",
        "enc = char_tfidfv.transform(char_tfidf_train_df['text']).toarray()\r\n",
        "char_tfidf_train_df = pd.concat([char_tfidf_train_df.reset_index(),pd.DataFrame(enc)],axis=1).drop(['level_0','index','text','sentencelen','charlen','upperlen'],axis=1)\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(char_tfidf_train_df.drop('author',axis=1), char_tfidf_train_df['author'],\r\n",
        "                                                    test_size=0.2, random_state=2021,\r\n",
        "                                                    stratify=char_tfidf_train_df['author']) \r\n",
        "xgb_model = xgb.XGBClassifier(num_class=5,objective='multi:softmax',tree_method='gpu_hist', gpu_id=0,n_estimators=10000)\r\n",
        "xgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['merror','mlogloss'],early_stopping_rounds=1000,verbose=False)\r\n",
        "lgb_model = lgb.LGBMClassifier(num_class=5,objective='multiclass',device_type='gpu',n_estimators=10000,early_stopping_round=1000)\r\n",
        "lgb_model.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric=['multi_error','multi_logloss'],verbose=False)\r\n",
        "ctb_model = ctb.CatBoostClassifier(n_estimators=10000,early_stopping_rounds=1000,task_type=\"GPU\",loss_function='MultiClass')\r\n",
        "ctb_model.fit(x_train,y_train,eval_set=[(x_test,y_test)],verbose=False)\r\n",
        "print(f'xgb : {xgb_model.best_iteration,xgb_model.score(x_test,y_test)}')\r\n",
        "print(f'lgb : {lgb_model.score(x_test,y_test)}')\r\n",
        "print(f'ctb : {ctb_model.get_best_iteration(),ctb_model.score(x_test,y_test)}')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xgb : (83, 0.8507462686567164)\n",
            "lgb : 0.8656716417910447\n",
            "ctb : (1064, 0.8059701492537313)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXblD1ynKT7p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}